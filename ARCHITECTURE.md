# ðŸ—ï¸ Multimodal Sentiment Analysis - System Architecture

## ðŸ“‹ Document Overview

This document provides a detailed technical architecture overview of the multimodal sentiment analysis system, including data flow, component interactions, and implementation details.

---

## ðŸŽ¯ System Overview

### **High-Level Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT LAYER                             â”‚
â”‚  Web Interface â”‚ Mobile App â”‚ API Clients â”‚ Third Party     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP/REST API
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  API GATEWAY                                â”‚
â”‚              FastAPI Server                                 â”‚
â”‚        â€¢ CORS handling                                      â”‚
â”‚        â€¢ Request validation                                 â”‚
â”‚        â€¢ File upload processing                             â”‚
â”‚        â€¢ Response formatting                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               PROCESSING LAYER                              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Speech     â”‚ â”‚   Feature    â”‚ â”‚    Model            â”‚ â”‚
â”‚  â”‚ Transcriptionâ”‚ â”‚  Extraction  â”‚ â”‚   Inference         â”‚ â”‚
â”‚  â”‚  (Whisper)   â”‚ â”‚  Pipeline    â”‚ â”‚  (PyTorch)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA LAYER                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Temporary    â”‚ â”‚   Model      â”‚ â”‚    Configuration     â”‚ â”‚
â”‚  â”‚ File Storage â”‚ â”‚  Weights     â”‚ â”‚       Data           â”‚ â”‚
â”‚  â”‚              â”‚ â”‚ (.pth files) â”‚ â”‚                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Data Flow Architecture

### **1. Request Processing Flow**

```
ðŸ“¤ Client Upload
    â”‚
    â–¼
ðŸ” Input Validation
    â”‚ â€¢ File type checking
    â”‚ â€¢ Size validation  
    â”‚ â€¢ Format verification
    â–¼
ðŸ’¾ Temporary Storage
    â”‚ â€¢ Save uploaded files
    â”‚ â€¢ Generate unique IDs
    â”‚ â€¢ Clean up policy
    â–¼
ðŸŽµ Audio Extraction
    â”‚ â€¢ FFmpeg processing
    â”‚ â€¢ Format conversion
    â”‚ â€¢ Quality optimization
    â–¼
ðŸ—£ï¸ Speech Transcription
    â”‚ â€¢ Whisper model
    â”‚ â€¢ Word-level timestamps
    â”‚ â€¢ Language detection
    â–¼
âœ‚ï¸ Segmentation
    â”‚ â€¢ Split by utterances
    â”‚ â€¢ Time boundaries
    â”‚ â€¢ Text alignment
    â–¼
ðŸŽ¯ Feature Extraction
    â”‚ â”œâ”€â”€ Text: BERT tokenization
    â”‚ â”œâ”€â”€ Video: Frame extraction
    â”‚ â””â”€â”€ Audio: Mel-spectrograms
    â–¼
ðŸ§  Model Inference
    â”‚ â€¢ Multimodal fusion
    â”‚ â€¢ Emotion prediction
    â”‚ â€¢ Sentiment analysis
    â–¼
ðŸ“Š Response Generation
    â”‚ â€¢ Confidence scoring
    â”‚ â€¢ JSON formatting
    â”‚ â€¢ Error handling
    â–¼
ðŸ“¤ Client Response
```

---

## ðŸ§  Neural Network Architecture

### **Component Hierarchy**

```
MultimodalSentimentModel
â”œâ”€â”€ TextEncoder (BERT-based)
â”‚   â”œâ”€â”€ bert: AutoModel('bert-base-uncased')
â”‚   â”‚   â”œâ”€â”€ embeddings: BertEmbeddings
â”‚   â”‚   â”œâ”€â”€ encoder: BertEncoder (12 layers)
â”‚   â”‚   â””â”€â”€ pooler: BertPooler
â”‚   â””â”€â”€ projection: Linear(768 â†’ 128)
â”‚
â”œâ”€â”€ VideoEncoder (3D CNN)
â”‚   â””â”€â”€ r3d: R3D_18
â”‚       â”œâ”€â”€ stem: Conv3d + BatchNorm3d + ReLU
â”‚       â”œâ”€â”€ layer1-4: BasicBlock3d layers
â”‚       â”œâ”€â”€ avgpool: AdaptiveAvgPool3d
â”‚       â””â”€â”€ fc: Linear(512 â†’ 128)
â”‚
â”œâ”€â”€ AudioEncoder (1D CNN)
â”‚   â”œâ”€â”€ conv_layers: Sequential
â”‚   â”‚   â”œâ”€â”€ Conv1d(64 â†’ 64, kernel=3)
â”‚   â”‚   â”œâ”€â”€ BatchNorm1d(64)
â”‚   â”‚   â”œâ”€â”€ ReLU + MaxPool1d(2)
â”‚   â”‚   â”œâ”€â”€ Conv1d(64 â†’ 128, kernel=3)
â”‚   â”‚   â””â”€â”€ AdaptiveAvgPool1d(1)
â”‚   â””â”€â”€ projection: Linear(128 â†’ 128)
â”‚
â”œâ”€â”€ fusion_layer: Sequential
â”‚   â”œâ”€â”€ Linear(384 â†’ 256)
â”‚   â”œâ”€â”€ BatchNorm1d(256)
â”‚   â”œâ”€â”€ ReLU()
â”‚   â”œâ”€â”€ Dropout(0.3)
â”‚   â””â”€â”€ Linear(256 â†’ 256)
â”‚
â”œâ”€â”€ emotion_classifier: Sequential
â”‚   â”œâ”€â”€ Linear(256 â†’ 128)
â”‚   â”œâ”€â”€ ReLU()
â”‚   â”œâ”€â”€ Dropout(0.2)
â”‚   â””â”€â”€ Linear(128 â†’ 7)  # 7 emotion classes
â”‚
â””â”€â”€ sentiment_classifier: Sequential
    â”œâ”€â”€ Linear(256 â†’ 128)
    â”œâ”€â”€ ReLU()
    â”œâ”€â”€ Dropout(0.2)
    â””â”€â”€ Linear(128 â†’ 3)  # 3 sentiment classes
```

### **Tensor Flow Dimensions**

```
Input Processing:
â”œâ”€â”€ Text: [batch, seq_len] â†’ tokenizer â†’ [batch, 512] input_ids + attention_mask
â”œâ”€â”€ Video: [batch, 30, 3, 224, 224] frames 
â””â”€â”€ Audio: [batch, 64, 300] mel-spectrogram

Feature Extraction:
â”œâ”€â”€ Text: [batch, 512] â†’ BERT â†’ [batch, 768] â†’ projection â†’ [batch, 128]
â”œâ”€â”€ Video: [batch, 30, 3, 224, 224] â†’ R3D â†’ [batch, 512] â†’ fc â†’ [batch, 128]
â””â”€â”€ Audio: [batch, 64, 300] â†’ 1D CNN â†’ [batch, 128] â†’ projection â†’ [batch, 128]

Fusion & Classification:
â”œâ”€â”€ Concatenation: [batch, 128+128+128] = [batch, 384]
â”œâ”€â”€ Fusion: [batch, 384] â†’ fusion_layer â†’ [batch, 256]
â”œâ”€â”€ Emotions: [batch, 256] â†’ emotion_classifier â†’ [batch, 7]
â””â”€â”€ Sentiments: [batch, 256] â†’ sentiment_classifier â†’ [batch, 3]

Output Processing:
â”œâ”€â”€ Softmax: logits â†’ probabilities
â”œâ”€â”€ ArgMax: probabilities â†’ class indices
â””â”€â”€ Mapping: indices â†’ label strings
```

---

## âš™ï¸ Component Details

### **1. Speech Processing Pipeline**

```python
# Architecture Flow
Video File
    â”‚ FFmpeg
    â–¼
Audio Stream (16kHz, mono)
    â”‚ Whisper
    â–¼
Transcription + Timestamps
    â”‚ Segmentation Algorithm
    â–¼
Utterances with Time Boundaries
    â”‚ For each utterance:
    â”œâ”€â”€ Extract corresponding video frame
    â”œâ”€â”€ Extract audio segment  
    â”œâ”€â”€ Process text through BERT
    â””â”€â”€ Combine for multimodal analysis
```

**Key Components:**
- **Whisper Model**: `whisper.load_model("base")` for speech-to-text
- **FFmpeg Integration**: Audio extraction and format conversion
- **Timestamp Alignment**: Word-level precision for segmentation
- **Utterance Detection**: Natural pause-based splitting

### **2. Multimodal Feature Extraction**

#### **Text Processing**
```python
# BERT Tokenization and Encoding
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

# Process text
inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors="pt")
outputs = model(**inputs)
text_features = outputs.pooler_output  # [CLS] token representation
```

#### **Video Processing**  
```python
# Frame Extraction and 3D CNN
cap = cv2.VideoCapture(video_path)
frames = []
for i in range(30):  # Extract 30 frames
    ret, frame = cap.read()
    frame = cv2.resize(frame, (224, 224))
    frames.append(frame)

video_tensor = torch.FloatTensor(frames).permute(0, 3, 1, 2)  # [30, 3, 224, 224]
video_features = r3d_model(video_tensor.unsqueeze(0))  # Add batch dimension
```

#### **Audio Processing**
```python
# Mel-Spectrogram Generation
audio, sr = librosa.load(audio_path, sr=16000)
mel_spec = librosa.feature.melspectrogram(
    y=audio, 
    sr=sr, 
    n_mels=64, 
    n_fft=2048, 
    hop_length=512
)
audio_features = torch.FloatTensor(mel_spec).unsqueeze(0)  # Add batch dimension
```

---

## ðŸ”§ System Configuration

### **Environment Variables**
```python
# Device Configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
USE_GPU = torch.cuda.is_available()

# Model Paths
MODEL_PATH = "path/to/model.pth"
WHISPER_MODEL = "base"  # or "small", "medium", "large"

# Processing Parameters
MAX_FRAMES = 30
VIDEO_SIZE = (224, 224)
AUDIO_SAMPLE_RATE = 16000
MEL_BINS = 64
TEXT_MAX_LENGTH = 512

# API Configuration
API_HOST = "127.0.0.1"
API_PORT = 8000
CORS_ORIGINS = ["http://localhost:3000"]
```

### **Memory Management**
```python
# Automatic cleanup of temporary files
@contextmanager
def temp_file_manager(suffix='.mp4'):
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        yield temp_file.name
    finally:
        os.unlink(temp_file.name)

# GPU memory optimization
torch.cuda.empty_cache()  # Clear GPU cache
```

---

## ðŸ“Š Performance Characteristics

### **Throughput Metrics**
- **CPU Processing**: 10-30 seconds per video (2-3 minutes)
- **GPU Processing**: 3-10 seconds per video (2-3 minutes)  
- **Concurrent Requests**: FastAPI supports async processing
- **Memory Usage**: 2-4GB RAM during processing

### **Scalability Considerations**
- **Horizontal Scaling**: Multiple server instances behind load balancer
- **Vertical Scaling**: GPU acceleration, more RAM
- **Caching Strategy**: Redis for frequent requests
- **Database Integration**: Store results for repeat analysis

### **Quality Metrics**
- **Speech Recognition**: Depends on audio quality and language
- **Emotion Detection**: Model accuracy varies by domain
- **Sentiment Analysis**: Generally high accuracy for clear text
- **Confidence Scores**: Provided for each prediction

---

## ðŸ›¡ï¸ Error Handling & Resilience

### **Error Categories**

1. **Input Validation Errors**
   - Invalid file formats
   - File size limitations
   - Corrupted uploads

2. **Processing Errors**
   - FFmpeg failures
   - Whisper transcription errors
   - Model inference failures

3. **System Errors**
   - Out of memory
   - Disk space issues
   - Network timeouts

### **Recovery Strategies**

```python
# Graceful degradation
try:
    utterances = process_video_segments(video_path)
except Exception as e:
    logger.warning(f"Segmentation failed: {e}")
    # Fallback to single-segment analysis
    utterances = create_fallback_utterance(video_path)

# Retry mechanisms
@retry(max_attempts=3, backoff=2.0)
def transcribe_audio(audio_path):
    return whisper_model.transcribe(audio_path)
```

---

## ðŸ”„ Future Architecture Enhancements

### **Planned Improvements**
1. **Real-time Processing**: WebSocket support for live video streams
2. **Model Optimization**: Quantization and pruning for faster inference
3. **Multi-language Support**: Extended Whisper model integration
4. **Cloud Integration**: AWS/GCP deployment with auto-scaling
5. **Advanced Caching**: Intelligent result caching and invalidation

### **Extension Points**
- **Plugin Architecture**: Support for custom emotion models
- **API Versioning**: Backward compatibility for API changes
- **Monitoring Integration**: Prometheus/Grafana metrics
- **Security Layer**: Authentication and rate limiting

---

## ðŸ“ Configuration Files

### **requirements.txt Structure**
```
# Core ML Framework
torch>=1.13.0
torchvision>=0.14.0
torchaudio>=0.13.0

# Transformers & NLP
transformers>=4.21.0
openai-whisper>=20230314

# Web Framework
fastapi>=0.95.0
uvicorn>=0.20.0

# Media Processing
opencv-python>=4.7.0
librosa>=0.9.0
ffmpeg-python>=0.2.0

# Utilities
numpy>=1.21.0
requests>=2.28.0
```

This architecture provides a robust, scalable foundation for multimodal sentiment analysis with clear separation of concerns and extensibility for future enhancements.
