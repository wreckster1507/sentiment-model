# üé≠ Multimodal Sentiment Analysis System - Production Ready

## üìã Table of Contents
- [Overview](#overview)
- [System Architecture](#system-architecture) 
- [Key Features](#key-features)
- [File Structure](#file-structure)
- [API Documentation](#api-documentation)
- [Installation & Setup](#installation--setup)
- [Usage Examples](#usage-examples)
- [Production Deployment](#production-deployment)
- [Technical Details](#technical-details)
- [Troubleshooting](#troubleshooting)

## üîç Overview

This is a **production-ready multimodal sentiment analysis system** that provides real-time sentiment and emotion analysis from video content. The system processes video files and returns time-segmented analysis with speech transcription.

**üîó Model Download:** The trained model is automatically downloaded from Google Drive during first startup.  
**üìé Model Link:** [Download Model](https://drive.google.com/drive/folders/1E6jn1B_emVdMvo9p84RNBs4C9QTWTMPd?usp=drive_link)

### üéØ What It Does:
- **Video Upload**: Accepts video files via web interface or API
- **Speech-to-Text**: Transcribes dialogue using OpenAI Whisper
- **Time Segmentation**: Breaks video into utterances with timestamps
- **Multimodal Analysis**: Analyzes text, video, and audio features
- **Detailed Results**: Returns emotions and sentiments for each segment

### üìä Analysis Categories:

**Emotions (7 classes):** Anger, Disgust, Fear, Joy, Neutral, Sadness, Surprise  
**Sentiments (3 classes):** Negative, Neutral, Positive

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VIDEO INPUT                              ‚îÇ
‚îÇ               (MP4, AVI, MOV, etc.)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                SPEECH TRANSCRIPTION                         ‚îÇ
‚îÇ             (OpenAI Whisper Model)                          ‚îÇ
‚îÇ         ‚Ä¢ Extracts audio from video                        ‚îÇ
‚îÇ         ‚Ä¢ Transcribes speech to text                       ‚îÇ
‚îÇ         ‚Ä¢ Provides word-level timestamps                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               TIME SEGMENTATION                             ‚îÇ
‚îÇ          ‚Ä¢ Splits into utterances (speech segments)        ‚îÇ
‚îÇ          ‚Ä¢ Each segment: start_time, end_time, text         ‚îÇ
‚îÇ          ‚Ä¢ Typical result: 30-60 segments per video        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             MULTIMODAL FEATURE EXTRACTION                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    Text      ‚îÇ      Video       ‚îÇ         Audio            ‚îÇ
‚îÇ   (BERT)     ‚îÇ   (R3D CNN)      ‚îÇ   (Mel-Spectrogram)      ‚îÇ
‚îÇ              ‚îÇ                  ‚îÇ                          ‚îÇ
‚îÇ ‚Ä¢ Tokenize   ‚îÇ ‚Ä¢ Extract frame  ‚îÇ ‚Ä¢ Extract audio          ‚îÇ
‚îÇ ‚Ä¢ BERT       ‚îÇ   at segment     ‚îÇ ‚Ä¢ Generate features      ‚îÇ
‚îÇ   encoding   ‚îÇ   midpoint       ‚îÇ ‚Ä¢ Spectral analysis      ‚îÇ
‚îÇ ‚Ä¢ [CLS] token‚îÇ ‚Ä¢ R3D-18 model   ‚îÇ ‚Ä¢ Temporal patterns      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ              ‚îÇ                      ‚îÇ
         ‚ñº              ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               SENTIMENT PREDICTION                          ‚îÇ
‚îÇ            (Custom Multimodal Model)                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Input: Combined text + video + audio features             ‚îÇ
‚îÇ  Output: 7 emotions + 3 sentiments per segment             ‚îÇ
‚îÇ  Architecture: Fusion ‚Üí FC layers ‚Üí Softmax                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   JSON RESPONSE                             ‚îÇ
‚îÇ    {                                                        ‚îÇ
‚îÇ      "analysis": {                                          ‚îÇ
‚îÇ        "utterances": [                                      ‚îÇ
‚îÇ          {                                                  ‚îÇ
‚îÇ            "start_time": 3.8,                               ‚îÇ
‚îÇ            "end_time": 5.4,                                 ‚îÇ
‚îÇ            "text": "Yeah, everybody's here.",              ‚îÇ
‚îÇ            "emotions": [...],                               ‚îÇ
‚îÇ            "sentiments": [...]                              ‚îÇ
‚îÇ          },                                                 ‚îÇ
‚îÇ          ...                                                ‚îÇ
‚îÇ        ]                                                    ‚îÇ
‚îÇ      }                                                      ‚îÇ
‚îÇ    }                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## ‚ú® Key Features

### üöÄ **Production Ready**
- **FastAPI Server** with CORS support
- **GPU/CPU Auto-detection** with fallback
- **Error Handling** and logging
- **Health Check** endpoint
- **Memory Management** with cleanup

### üéØ **Advanced Processing**
- **Real-time Speech Transcription** (Whisper)
- **Time-segmented Analysis** (30-60 segments per video)
- **Multimodal Fusion** (text + video + audio)
- **Batch Processing** support

### üîß **Easy Integration**
- **REST API** with JSON responses
- **Web Interface** compatible
- **File Upload** support
- **Cross-platform** (Windows/Linux/Mac)
‚îÇ              ‚îÇ 128 dim output   ‚îÇ 128 dim output           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ              ‚îÇ                      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   FUSION LAYER      ‚îÇ
            ‚îÇ                     ‚îÇ
            ‚îÇ Concatenate(3√ó128)  ‚îÇ
            ‚îÇ Linear(384‚Üí256)     ‚îÇ
            ‚îÇ BatchNorm + ReLU    ‚îÇ
            ‚îÇ Dropout(0.3)        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ CLASSIFICATION HEAD ‚îÇ
            ‚îÇ                     ‚îÇ
            ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
            ‚îÇ ‚îÇ Emotion Head    ‚îÇ ‚îÇ
            ‚îÇ ‚îÇ 256‚Üí64‚Üí7 classes‚îÇ ‚îÇ
            ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
            ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
            ‚îÇ ‚îÇ Sentiment Head  ‚îÇ ‚îÇ
            ‚îÇ ‚îÇ 256‚Üí64‚Üí3 classes‚îÇ ‚îÇ
            ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üíª How the Code Works

## üìÅ File Structure

```
c:\sentiment-model\
‚îú‚îÄ‚îÄ üìÑ models.py                     # Model architecture definitions
‚îú‚îÄ‚îÄ üìÑ local_deploy_v2.py           # Production FastAPI server (MAIN)
‚îú‚îÄ‚îÄ üìÑ local_inference.py           # Processing utilities & feature extraction
‚îú‚îÄ‚îÄ üìÑ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ üìÑ README.md                    # This documentation
‚îú‚îÄ‚îÄ üìÑ install_ffmpeg_windows.py   # FFmpeg setup script
‚îú‚îÄ‚îÄ üìÑ performance_analysis.py     # Performance monitoring tools
‚îî‚îÄ‚îÄ üìÅ aisentiment/                # Virtual environment
    ‚îú‚îÄ‚îÄ üìÅ Scripts/                # Python executables
    ‚îú‚îÄ‚îÄ üìÅ Lib/                    # Installed packages (PyTorch, FastAPI, etc.)
    ‚îî‚îÄ‚îÄ üìÑ pyvenv.cfg              # Environment configuration
```

### üîë **Core Files:**

#### **`local_deploy_v2.py`** - Main Production Server
- FastAPI application with CORS support
- Speech transcription with OpenAI Whisper
- Time-segmented video analysis
- REST API endpoints (`/predict`, `/health`)
- GPU/CPU auto-detection
- Comprehensive error handling

#### **`models.py`** - Neural Network Architecture  
- `MultimodalSentimentModel` class
- Text, Video, and Audio encoders
- Feature fusion layers
- Emotion and sentiment classifiers

#### **`local_inference.py`** - Processing Pipeline
- Video frame extraction (`VideoProcessor`)
- Audio feature extraction (`AudioProcessor`) 
- Text tokenization (`process_text_local`)
- Feature preprocessing utilities

#### **`requirements.txt`** - Dependencies
- PyTorch, FastAPI, Transformers
- OpenCV, Whisper, NumPy
- All necessary packages for production

## üöÄ API Documentation

### **Primary Endpoint: `/predict`**

**Method:** `POST`  
**Content-Type:** `multipart/form-data`

**Parameters:**
- `video_file` (file): Video file to analyze
- `text` (optional): Additional text context
- `audio_file` (optional): Separate audio file

**Response Format:**
```json
{
  "analysis": {
    "utterances": [
      {
        "start_time": 3.8,
        "end_time": 5.4,
        "text": "Yeah, yeah, everybody's here.",
        "emotions": [
          {"label": "neutral", "confidence": 0.59},
          {"label": "joy", "confidence": 0.22},
          {"label": "anger", "confidence": 0.07},
          {"label": "sadness", "confidence": 0.05},
          {"label": "surprise", "confidence": 0.03},
          {"label": "fear", "confidence": 0.02},
          {"label": "disgust", "confidence": 0.02}
        ],
        "sentiments": [
          {"label": "neutral", "confidence": 0.63},
          {"label": "positive", "confidence": 0.22},
          {"label": "negative", "confidence": 0.14}
        ]
      }
      // ... more utterances (typically 30-60 per video)
    ]
  }
}
```

### **Health Check: `/health`**

**Method:** `GET`  
**Response:** `{"status": "healthy", "model_loaded": true}`

## üîÑ Data Flow

### Local Deployment Flow:

```
1. üåê HTTP Request
   ‚îÇ
   ‚îú‚îÄ‚îÄ üìù Text: "I am happy!"
   ‚îú‚îÄ‚îÄ üé• Video: uploaded_video.mp4## üöÄ Installation & Setup

### Prerequisites
- **Python 3.8+** (tested on Python 3.12)
- **Windows 10/11** (or Linux/Mac with modifications)
- **8GB+ RAM** (16GB recommended)
- **GPU optional** (CUDA 11.8+ for acceleration)

### 1. **Environment Setup**
```bash
# Clone or download the project
cd c:\sentiment-model

# Create virtual environment (if not exists)
python -m venv aisentiment

# Activate environment
.\aisentiment\Scripts\activate  # Windows
# source aisentiment/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. **Install FFmpeg** (Required for audio processing)
```bash
# Option 1: Run the setup script
python install_ffmpeg_windows.py

# Option 2: Manual installation
# Download from https://ffmpeg.org/download.html
# Add to system PATH
```

### 3. **Model Setup**
Ensure your trained model file is available:
```
C:\Users\[username]\Downloads\model-[timestamp]\model\model.pth
```

Update the path in `local_deploy_v2.py` if different:
```python
model_path = r"C:\Users\sarthu\Downloads\model-20250614T082123Z-1-001\model\model.pth"
```

### 4. **Start the Server**
```bash
# Activate environment
.\aisentiment\Scripts\activate

# Start production server
python local_deploy_v2.py

# Server will start on http://127.0.0.1:8000
```

## üíª Usage Examples

### **Web Interface Integration**
```javascript
// Upload video for analysis
const formData = new FormData();
formData.append('video_file', videoFile);

const response = await fetch('http://127.0.0.1:8000/predict', {
  method: 'POST',
  body: formData
});

const result = await response.json();
console.log(`Found ${result.analysis.utterances.length} utterances`);

// Process each segment
result.analysis.utterances.forEach(utterance => {
  console.log(`${utterance.start_time}s-${utterance.end_time}s: "${utterance.text}"`);
  console.log(`Top emotion: ${utterance.emotions[0].label} (${utterance.emotions[0].confidence})`);
  console.log(`Top sentiment: ${utterance.sentiments[0].label} (${utterance.sentiments[0].confidence})`);
});
```

### **Python API Client**
```python
import requests

# Test with video file
video_path = "path/to/your/video.mp4"

with open(video_path, "rb") as video_file:
    files = {"video_file": video_file}
    response = requests.post("http://127.0.0.1:8000/predict", files=files)
    
    result = response.json()
    utterances = result["analysis"]["utterances"]
    
    print(f"Analysis complete: {len(utterances)} segments found")
    
    for utterance in utterances:
        print(f"\n{utterance['start_time']:.1f}s - {utterance['end_time']:.1f}s:")
        print(f"Text: {utterance['text']}")
        print(f"Top emotion: {utterance['emotions'][0]['label']} ({utterance['emotions'][0]['confidence']:.3f})")
        print(f"Top sentiment: {utterance['sentiments'][0]['label']} ({utterance['sentiments'][0]['confidence']:.3f})")
```

### **cURL Example**
```bash
# Test health endpoint
curl http://127.0.0.1:8000/health

# Upload video for analysis
curl -X POST "http://127.0.0.1:8000/predict" \
     -F "video_file=@sample_video.mp4"
```

## üè≠ Production Deployment

### **Performance Optimization**
- **GPU Acceleration**: Install CUDA PyTorch for 3-5x speed improvement
- **Memory Management**: Server automatically cleans up temporary files
- **Concurrent Processing**: FastAPI supports async requests
- **Caching**: Consider Redis for frequently accessed videos

### **Scaling Options**
- **Docker**: Containerize for easy deployment
- **Load Balancer**: Use nginx for multiple server instances
- **Cloud Deployment**: AWS EC2, Google Cloud, or Azure
- **Kubernetes**: For large-scale production

### **Monitoring**
```python
# Check server status
response = requests.get("http://127.0.0.1:8000/health")
print(response.json())  # {"status": "healthy", "model_loaded": true}
```

### Prerequisites
- Python 3.8+
- FFmpeg (for audio/video processing)
- 4GB+ RAM recommended

### Steps

1. **Activate Virtual Environment:**
   ```bash
   # Windows Command Prompt
   aisentiment\Scripts\activate.bat
   
   # Windows PowerShell
   .\aisentiment\Scripts\Activate.ps1
   ```

2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start the Server:**
   ```bash
   # Option 1: Direct Python
   python local_deploy.py
   
   # Option 2: Batch script
   start_server.bat
   
   # Option 3: PowerShell script  
   .\start_server.ps1
   ```

4. **Verify Installation:**
   ```bash
   python test_api.py
   ```

## üí° Usage Examples

### 1. Using Python Requests

```python
import requests

# Text prediction
response = requests.post(
    "http://127.0.0.1:8000/predict_text",
    data={"text": "I love this movie!"}
)
print(response.json())

# Multimodal prediction
with open("video.mp4", "rb") as video, open("audio.wav", "rb") as audio:
    files = {"video_file": video, "audio_file": audio}
    data = {"text": "This scene is amazing!"}
    response = requests.post(
        "http://127.0.0.1:8000/predict",
        files=files,
        data=data
    )
    print(response.json())
```

### 2. Using cURL

```bash
# Text prediction
curl -X POST "http://127.0.0.1:8000/predict_text" \
     -d "text=I am feeling great today!"

# File upload
curl -X POST "http://127.0.0.1:8000/predict" \
     -F "text=This is wonderful!" \
     -F "video_file=@video.mp4" \
     -F "audio_file=@audio.wav"
```

## üîß Technical Deep Dive

### Model Loading Process

1. **Model Architecture Creation:**
   ```python
   model = MultimodalSentimentModel()  # Empty model with random weights
   ```

2. **Loading Trained Weights:**
   ```python
   state_dict = torch.load(model_path, map_location='cpu')  # Load from model.pth
   model.load_state_dict(state_dict)                       # Apply weights
   model.eval()                                            # Set to evaluation mode
   ```

3. **Model Path Resolution:**
   ```python
   model_path = r"C:\Users\sarthu\Downloads\model-20250614T082123Z-1-001\model\model.pth"
   ```

### Input Processing Details

#### Text Processing:
```python
# Tokenization with BERT
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer(
    text,
    padding="max_length",      # Pad to 512 tokens
    truncation=True,           # Cut if longer than 512
    max_length=512,
    return_tensors="pt"        # Return PyTorch tensors
)
# Output: {input_ids: [1, 512], attention_mask: [1, 512]}
```

#### Video Processing:
```python
# Frame extraction and preprocessing
cap = cv2.VideoCapture(video_path)
frames = []
while len(frames) < 30:  # Extract exactly 30 frames
    ret, frame = cap.read()
    frame = cv2.resize(frame, (224, 224))  # Resize to 224x224
    frame = frame / 255.0                  # Normalize to [0,1]
    frames.append(frame)

# Convert to tensor: [30, 224, 224, 3] ‚Üí [30, 3, 224, 224]
video_tensor = torch.FloatTensor(frames).permute(0, 3, 1, 2)
```

#### Audio Processing:
```python
# Load and resample audio
waveform, sr = torchaudio.load(audio_path)
if sr != 16000:
    resampler = torchaudio.transforms.Resample(sr, 16000)
    waveform = resampler(waveform)

# Generate mel-spectrogram
mel_transform = torchaudio.transforms.MelSpectrogram(
    sample_rate=16000,
    n_mels=64,        # 64 mel-frequency bins
    n_fft=1024,       # FFT size
    hop_length=512    # Hop between frames
)
mel_spec = mel_transform(waveform)  # Shape: [1, 64, time_steps]

# Normalize and pad/truncate to 300 time steps
mel_spec = (mel_spec - mel_spec.mean()) / mel_spec.std()
mel_spec = pad_or_truncate(mel_spec, 300)  # Final: [1, 64, 300]
```

### Model Architecture Details

#### Text Encoder (BERT-based):
```python
class TextEncoder(nn.Module):
    def __init__(self):
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        # Freeze BERT parameters (transfer learning)
        for param in self.bert.parameters():
            param.requires_grad = False
        # Add projection layer
        self.projection = nn.Linear(768, 128)  # BERT hidden size ‚Üí 128
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooler_output = outputs.pooler_output  # [CLS] token representation
        return self.projection(pooler_output)   # Project to 128 dimensions
```

#### Video Encoder (3D CNN):
```python
class VideoEncoder(nn.Module):
    def __init__(self):
        # R3D-18: 3D ResNet for video understanding
        self.backbone = vision_models.video.r3d_18(pretrained=True)
        # Freeze pretrained features
        for param in self.backbone.parameters():
            param.requires_grad = False
        # Replace final layer
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Linear(num_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
    
    def forward(self, x):
        # Input: [batch, frames, channels, height, width]
        # Need: [batch, channels, frames, height, width] for R3D
        x = x.transpose(1, 2)
        return self.backbone(x)
```

#### Audio Encoder (1D CNN):
```python
class AudioEncoder(nn.Module):
    def __init__(self):
        self.conv_layers = nn.Sequential(
            # Process mel-spectrogram with 1D convolutions
            nn.Conv1d(64, 64, kernel_size=3),    # 64 mel-bins input
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            nn.Conv1d(64, 128, kernel_size=3),   # Increase features
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)              # Global average pooling
        )
        self.projection = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
  ## üîß Technical Details

### **Model Architecture**

#### **Text Encoder (BERT-based)**
```python
class TextEncoder(nn.Module):
    def __init__(self):
        self.bert = AutoModel.from_pretrained('bert-base-uncased')
        self.projection = nn.Linear(768, 128)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # [CLS] token representation
        return self.projection(pooled_output)   # Project to 128 dimensions
```

#### **Video Encoder (3D CNN)**
```python
class VideoEncoder(nn.Module):
    def __init__(self):
        self.r3d = r3d_18(pretrained=True)      # R3D-18 from torchvision
        self.r3d.fc = nn.Linear(512, 128)       # Replace final layer
    
    def forward(self, video_frames):
        # Input: [batch, frames=30, channels=3, height=224, width=224]
        # Output: [batch, 128]
        return self.r3d(video_frames)
```

#### **Audio Encoder (1D CNN)**
```python
class AudioEncoder(nn.Module):
    def __init__(self):
        self.conv_layers = nn.Sequential(
            nn.Conv1d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.AdaptiveAvgPool1d(1)
        )
        self.projection = nn.Linear(128, 128)
```

#### **Fusion & Classification**
```python
class MultimodalSentimentModel(nn.Module):
    def forward(self, text_inputs, video_frames, audio_features):
        # Extract features: each encoder outputs 128 dimensions
        text_feat = self.text_encoder(text_inputs)      # [batch, 128]
        video_feat = self.video_encoder(video_frames)   # [batch, 128]  
        audio_feat = self.audio_encoder(audio_features) # [batch, 128]
        
        # Concatenate features
        combined = torch.cat([text_feat, video_feat, audio_feat], dim=1)  # [batch, 384]
        
        # Fusion and classification
        fused = self.fusion_layer(combined)                    # [batch, 256]
        emotions = self.emotion_classifier(fused)              # [batch, 7]
        sentiments = self.sentiment_classifier(fused)          # [batch, 3]
        
        return {'emotions': emotions, 'sentiments': sentiments}
```

### **Speech Processing Pipeline**

1. **Audio Extraction**: FFmpeg extracts audio from video
2. **Whisper Transcription**: OpenAI Whisper converts speech to text with timestamps
3. **Segmentation**: Speech is split into utterances based on pauses
4. **Time Alignment**: Each utterance gets precise start/end times

### **Performance Metrics**

- **Throughput**: ~30-60 utterances per 2-3 minute video
- **Latency**: 10-30 seconds per video (CPU), 3-10 seconds (GPU)
- **Memory**: ~2-4GB RAM usage during processing
- **Accuracy**: Depends on training data and domain

## üîß Troubleshooting

### **Common Issues & Solutions**

#### **1. Server Won't Start**
```
Error: Model file not found
```
**Solution:**
- Check model path in `local_deploy_v2.py` line ~292
- Ensure `model.pth` exists at the specified location
- Update path to match your downloaded model

#### **2. CORS Errors in Browser**
```
Access to fetch blocked by CORS policy
```
**Solution:**
- Server includes CORS middleware for `localhost:3000`
- Add your frontend URL to allowed origins in `local_deploy_v2.py`

#### **3. FFmpeg Not Found**
```
[WinError 2] The system cannot find the file specified
```
**Solution:**
```bash
# Run the installer
python install_ffmpeg_windows.py

# Or install manually and add to PATH
```

#### **4. Out of Memory**
```
RuntimeError: out of memory
```
**Solution:**
- Model runs on CPU by default (no GPU memory issues)
- Ensure 8GB+ system RAM available
- Close other applications if needed

#### **5. Import Errors**
```
ModuleNotFoundError: No module named 'transformers'
```
**Solution:**
```bash
# Activate virtual environment
.\aisentiment\Scripts\activate

# Reinstall dependencies
pip install -r requirements.txt
```

#### **6. Empty Response from API**
```
{"analysis": {"utterances": []}}
```
**Solutions:**
- Check video file is valid and contains audio
- Ensure video is not corrupted
- Try with a different video format (MP4 recommended)

### **Performance Tuning**

#### **GPU Acceleration** (Optional)
```bash
# Install CUDA-enabled PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### **Memory Optimization**
- Server automatically cleans up temporary files
- Processing is done in segments to manage memory
- Large videos are handled efficiently

#### **Speed Optimization**
- **GPU**: 3-5x faster processing
- **SSD Storage**: Faster file I/O
- **More RAM**: Better caching

### **Logging & Debug**

Enable detailed logging by modifying `local_deploy_v2.py`:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Check server logs for detailed processing information and error traces.

---

## üìö Additional Resources

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Detailed system architecture documentation
- **[Model Training Guide]** - How the model was trained (if available)
- **[API Reference]** - Complete API documentation with examples
- **[Performance Benchmarks]** - Speed and accuracy metrics

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## üéâ Summary

You now have a **production-ready multimodal sentiment analysis system** that:

‚úÖ **Processes video files** with speech transcription  
‚úÖ **Returns time-segmented analysis** (30-60 utterances per video)  
‚úÖ **Provides emotion & sentiment predictions** for each segment  
‚úÖ **Includes a REST API** ready for web integration  
‚úÖ **Supports both CPU and GPU** processing  
‚úÖ **Handles CORS** for web browser compatibility  
‚úÖ **Has comprehensive error handling** and logging  

**Perfect for integration with web applications, mobile apps, or other services requiring video sentiment analysis!** üöÄ
{"analysis": {"utterances": []}}
```
**Solutions:**
- Check video file is valid and contains audio
- Ensure video is not corrupted
- Try with a different video format (MP4 recommended)

### **Performance Tuning**

#### **GPU Acceleration** (Optional)
```bash
# Install CUDA-enabled PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### **Memory Optimization**
- Server automatically cleans up temporary files
- Processing is done in segments to manage memory
- Large videos are handled efficiently

#### **Speed Optimization**
- **GPU**: 3-5x faster processing
- **SSD Storage**: Faster file I/O
- **More RAM**: Better caching

### **Logging & Debug**

Enable detailed logging by modifying `local_deploy_v2.py`:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Check server logs for detailed processing information and error traces.
- **Memory Requirements:** ~2-4GB RAM during inference
- **Processing Speed:** 
  - Text-only: ~100ms
  - Multimodal: ~500-1000ms depending on file sizes

### Debug Mode

To enable detailed logging, modify `local_deploy.py`:
```python
logging.basicConfig(level=logging.DEBUG)  # Change from INFO to DEBUG
```

## üéØ Key Features

‚úÖ **Multimodal Input:** Supports text, video, and audio simultaneously  
‚úÖ **Flexible Input:** Can work with any combination of modalities  
‚úÖ **Local Deployment:** No cloud dependencies, runs entirely offline  
‚úÖ **REST API:** Standard HTTP endpoints for easy integration  
‚úÖ **Confidence Scores:** Provides probability distributions for all classes  
‚úÖ **Real-time Processing:** Fast inference suitable for interactive applications  
‚úÖ **Pre-trained Models:** Uses BERT and R3D pre-trained on large datasets  

## üìä Model Performance

Based on your test results, the model shows:
- **High confidence** for clear emotional expressions (90%+)
- **Balanced predictions** for ambiguous cases
- **Realistic probability distributions** across classes
- **Proper sentiment-emotion alignment** (joy ‚Üî positive, sadness ‚Üî negative)

---

**üéâ Congratulations!** You now have a fully functional multimodal sentiment analysis system running locally. The model successfully combines three different types of AI (NLP, Computer Vision, and Audio Processing) into a single, powerful prediction system.
